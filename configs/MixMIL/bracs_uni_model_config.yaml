data:
  dataset_config: /home/ubuntu/Code/custom_framework/dataset_dependent_configs/bracs/bracs_coarse_univ1_dt_config.yaml
  artifact_dir: /home/ubuntu/Code/custom_framework/dataset_dependent_configs/bracs/out/mixmil/bracs_coarse_univ1_20x
  data_dims: 1024    # Embeddings dimensions Resnet50:1024, CONCH: 512, UNIv1: 1024, HOPTIMUS: 1536
  use_h5: True
  train_frac: 1.0 # fraction of training data to use [.1, .10, .25, .50, .75, 1.0]
  seed_mode: True # for multiple seed experiments-not used with k-fold
  split: 0 # or fold number for cross-validation

model:
  attention: mixmil # [gp, clam, transmil, abmil, dgrmil, mixmil]
  variant: null # [sb, mb] for single or multi branch
  model_size: null
  attn_branches: null # Number of attention branches-explain later
  feature_extractor: resnet50_lvl1 # [resnet50, conch, univ1, hoptimus]
  gate: null
  embed_dim: 1024 # for mixmil Q (input dimension)
  proj_dim: 512
  n_trials: 1
  # intermediate layers to reduce dimensionality of features i.e. L and M inside the model
  hidden_layer1: null
  hidden_layer2: null
  dropout: null
  attn_branches: null
  instance_loss_fn: null
  bag_loss_fn: null
  bag_weight: null # weight coefficient for bag level loss i.e. bag_weight * bag_loss + (1-bag_weight) * instance_loss
  subtyping: null
  print_model: True

logging:
  save_predictions: True
  run_name: test_run  # Can be set to auto
  tracking_url: ./mlruns
  high_conf_metrics: False
  wandb: True
  project: BRACS  # Name of the project in wandb i.e. Dataset name
  model_ckpt_dir: ../experiments/mixmil # Path to the directory where the model checkpoints will be saved
  model_version: v2


seed: 0   # Seed for everything with the seed_torch function from main.py
phase: train # train, test

training:
  batch_size: 1  # Effective only for mixmil, for other models it is set to 1
  max_epochs: 50
  gpu_index: [1]
  strategy: auto
  precision: '32'
  patience: 200 # Early stopping patience
  optimizer: adam # [adam, sgd, adamw, lookahead_radam]
  reg: 5.e-3 #  weight decay regularization
  scheduler: lambda # [lamda, cosine, linearcosine]
  learning_rate: 1.e-4  # 0.0001 reasonable choice
  # stuff exclusively for lamda scheduler
  lr_decay_after_epoch: 20    # Tune the decay factors
  stop_decay_after_epoch: 21
  stop_decay_lr_value: 0.00002 # Stop decaying the learning rate after this lr value
  lr_decay_factor: 0.1
  min_delta: 0.0001
  # stuff exclusively for cosine/linearcosine scheduler
  min_lr: 1.e-5
  # stuff exclusively for linearcosine scheduler
  warmup_epochs: 20
  warmup_lr: 1.e-5
  # logging stuff
  lr_logging_interval: epoch  # Log learning rate ['step', 'epoch', None]
  lr_logging_frequency: 1 # Log learning rate every n steps/epochs
  log_weight_decay: True

testing:
  experiment_ckpt_dir: 

